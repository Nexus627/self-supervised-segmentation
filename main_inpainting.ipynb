{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "##############################\n",
    "#           U-NET\n",
    "##############################\n",
    "\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 16)\n",
    "        self.down2 = UNetDown(16, 32)\n",
    "        self.down3 = UNetDown(32, 64)\n",
    "        self.down4 = UNetDown(64, 128)\n",
    "        self.down5 = UNetDown(128, 128)\n",
    "        self.down6 = UNetDown(128, 256)\n",
    "        self.up1 = UNetUp(256, 128)\n",
    "        self.up2 = UNetUp(256, 128)\n",
    "        self.up3 = UNetUp(256, 64)\n",
    "        self.up4 = UNetUp(128, 32)\n",
    "        self.up5 = UNetUp(64, 16)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(32, out_channels, 3, 1, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # U-Net generator with skip connections from encoder to decoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        u1 = self.up1(d6, d5)\n",
    "        u2 = self.up2(u1, d4)\n",
    "        u3 = self.up3(u2, d3)\n",
    "        u4 = self.up4(u3, d2)\n",
    "        u5 = self.up5(u4, d1)\n",
    "\n",
    "        return self.final(u5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    #discriminator model\n",
    "    def __init__(self, in_channel=1):\n",
    "        super(Discriminator,self).__init__()\n",
    "        \n",
    "        self.t1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channel,out_channels=32,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.t2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.t3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.t4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.t5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=1,kernel_size=(3,3),stride=1,padding=1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.t1(x)\n",
    "        x=self.t2(x)\n",
    "        x=self.t3(x)\n",
    "        x=self.t4(x)\n",
    "        x=self.t5(x)\n",
    "        return x #output of discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "from PIL import Image\n",
    "from skimage import io, img_as_ubyte, exposure\n",
    "from glob import glob\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "epoch=0\n",
    "n_epochs=200\n",
    "dataset_name='HeLa_3D'\n",
    "dataset_folder=\"dataset/train\"\n",
    "val_folder=\"dataset/val\"\n",
    "batch_size=8\n",
    "lr=0.0002\n",
    "b1=0.5\n",
    "b2=0.999\n",
    "decay_epoch=100\n",
    "n_cpu=8\n",
    "img_size=256 \n",
    "channels=3\n",
    "sample_interval=200\n",
    "checkpoint_interval=5\n",
    "num_critic=50\n",
    "\n",
    "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)\n",
    "os.makedirs(\"saved_models/%s\" % dataset_name, exist_ok=True)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 0.95\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, img_size // 2 ** 4, img_size // 2 ** 4)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_pixelwise.cuda()\n",
    "\n",
    "if epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/%s/generator_%d.pth\" % (dataset_name, epoch)))\n",
    "    discriminator.load_state_dict(torch.load(\"saved_models/%s/discriminator_%d.pth\" % (dataset_name, epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, re_norm=True):\n",
    "        self.re_norm = re_norm\n",
    "        self.transform = transforms.Compose(transform)\n",
    "        self.files = sorted(glob(root+\"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = io.imread(self.files[index])\n",
    "        if self.re_norm:\n",
    "            img = exposure.rescale_intensity(img, in_range=(np.percentile(img, 0), np.percentile(img, 100)), out_range=(0, 1))\n",
    "        img = Image.fromarray(img_as_ubyte(img))\n",
    "        img= self.transform(img)\n",
    "        r_low = 0.02\n",
    "        r_high = 0.05\n",
    "        img_erased = transforms.RandomErasing(p=1, scale=(r_low, r_high), ratio=(0.25, 2))(img)\n",
    "        img_erased = transforms.RandomErasing(p=1, scale=(r_low, r_high), ratio=(0.25, 2))(img_erased)\n",
    "        img_erased = transforms.RandomErasing(p=1, scale=(r_low, r_high), ratio=(0.25, 2))(img_erased)\n",
    "        # img_erased = transforms.RandomErasing(p=1, scale=(r_low, r_high), ratio=(0.25, 2))(img_erased)\n",
    "        # img_erased = transforms.RandomErasing(p=1, scale=(r_low, r_high), ratio=(0.25, 2))(img_erased)\n",
    "        return img, img_erased\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "# Configure dataloaders\n",
    "transform = [\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomAffine(180, scale=(0.75, 1.5), shear=45),\n",
    "    transforms.RandomCrop(img_size),\n",
    "    transforms.ToTensor()]\n",
    "dataset = ImageDataset(root=dataset_folder, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0)\n",
    "\n",
    "val_transform = [\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    # transforms.RandomAffine(180, scale=(0.75, 1.5), shear=45),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor()]\n",
    "val_dataset = ImageDataset(root=val_folder, transform=val_transform)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=0)\n",
    "\n",
    "# Tensor type\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    real_A = Variable(imgs[1].type(Tensor))\n",
    "    ### centercrop\n",
    "    # real_A[:, :, int(img_size/4):int(3*img_size/4), int(img_size/4):int(3*img_size/4)] = 0\n",
    "    real_B = Variable(imgs[0].type(Tensor))\n",
    "    fake_B = generator(real_A)\n",
    "    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "    save_image(img_sample, \"images/%s/%s.png\" % (dataset_name, batches_done), nrow=4, normalize=True)\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "for epoch in range(epoch, n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Model inputs\n",
    "        real_A = Variable(batch[1].type(Tensor)) # input\n",
    "        ### center crop\n",
    "        # real_A[:, :, int(img_size/4):int(3*img_size/4), int(img_size/4):int(3*img_size/4)] = 0\n",
    "        ### origial\n",
    "        real_B = Variable(batch[0].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = generator(real_A)\n",
    "        pred_fake = discriminator(fake_B)\n",
    "        loss_GAN = criterion_GAN(pred_fake, valid)\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = (1-lambda_pixel)*loss_GAN + lambda_pixel*loss_pixel\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i%num_critic==0:\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            pred_real = discriminator(real_B)\n",
    "            loss_real = criterion_GAN(pred_real, valid)\n",
    "\n",
    "            # Fake loss\n",
    "            pred_fake = discriminator(fake_B.detach())\n",
    "            loss_fake = criterion_GAN(pred_fake, fake)\n",
    "\n",
    "            # Total loss\n",
    "            loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = n_epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                n_epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                loss_pixel.item(),\n",
    "                loss_GAN.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % sample_interval == 0:\n",
    "            sample_images(batches_done)\n",
    "\n",
    "    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "        # Save model checkpoints\n",
    "        torch.save(generator.state_dict(), \"saved_models/%s/generator_%d.pth\" % (dataset_name, epoch))\n",
    "        torch.save(discriminator.state_dict(), \"saved_models/%s/discriminator_%d.pth\" % (dataset_name, epoch))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f9103d5badd20146b89a838eb17adf7ff85840da7a22fcf740f0eb9bda6816"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
